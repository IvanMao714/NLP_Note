
<!DOCTYPE HTML>
<html lang="zh-hans" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>NLM implementation · Gitbook使用教程</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        <meta name="author" content="Ivan Mao">
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-back-to-top-button/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-katex/katex.min.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-chapter-fold/chapter-fold.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-code/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-splitter/splitter.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-search-pro/search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-insert-logo/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
        <link rel="stylesheet" href="../styles/website.css">
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="../5 Attention mechanisms/Attention mechanisms.html" />
    
    
    <link rel="prev" href="../3 Backpropagation/Backpropagation(补充" />
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="输入并搜索" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" >
            
                <a target="_blank" href="http://ivanmao.mo.cn/Learning%20Notes/index.html">
            
                    
                    Ivan Mao Learning Book
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="../">
            
                <a href="../">
            
                    
                    NLP
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.1" data-path="../0 Intro/Intro.html">
            
                <a href="../0 Intro/Intro.html">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2" data-path="../1 Language Modeling/1 Language Modeling.html">
            
                <a href="../1 Language Modeling/1 Language Modeling.html">
            
                    
                    Language Modeling
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3" data-path="../2 Neural Language Models/Neural Language Models.html">
            
                <a href="../2 Neural Language Models/Neural Language Models.html">
            
                    
                    Neural Language Models
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.4" data-path="../3 Backpropagation/Backpropagation.html">
            
                <a href="../3 Backpropagation/Backpropagation.html">
            
                    
                    Backpropagation
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.4.1" data-path="../3 Backpropagation/Backpropagation(补充">
            
                <span>
            
                    
                    Backpropagation(补充)
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter active" data-level="1.2.5" data-path="NLM implementation.html">
            
                <a href="NLM implementation.html">
            
                    
                    NLM implementation
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6" data-path="../5 Attention mechanisms/Attention mechanisms.html">
            
                <a href="../5 Attention mechanisms/Attention mechanisms.html">
            
                    
                    Attention mechanisms
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7" data-path="../6 Transformer and sequence-to sequence learning/Transformer and sequence-to sequence learning.html">
            
                <a href="../6 Transformer and sequence-to sequence learning/Transformer and sequence-to sequence learning.html">
            
                    
                    Transformer and sequence-to sequence learning
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.8" data-path="../7 Transfer learning with neural language models/Transfer learning with neural language models.html">
            
                <a href="../7 Transfer learning with neural language models/Transfer learning with neural language models.html">
            
                    
                    Transfer learning with neural language models
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.9" data-path="../8 HuggingFace/HuggingFace.html">
            
                <a href="../8 HuggingFace/HuggingFace.html">
            
                    
                    HuggingFace
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            本书使用 GitBook 发布
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >NLM implementation</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h1 id="nlm-implementation">NLM implementation</h1>
<h2 id="&#x8F93;&#x5165;&#x6587;&#x672C;">&#x8F93;&#x5165;&#x6587;&#x672C;</h2>
<pre><code class="lang-python">sentences = [
             <span class="hljs-string">&apos;bob likes sheep&apos;</span>,
             <span class="hljs-string">&apos;alice is fast&apos;</span>,
             <span class="hljs-string">&apos;cs685 is fun&apos;</span>,
             <span class="hljs-string">&apos;i love lamp&apos;</span>
]
</code></pre>
<h2 id="&#x5217;&#x4E3E;&#x5BF9;&#x5E94;&#x7684;&#x7F16;&#x53F7;">&#x5217;&#x4E3E;&#x5BF9;&#x5E94;&#x7684;&#x7F16;&#x53F7;</h2>
<pre><code class="lang-python"><span class="hljs-comment"># given the first two words of each sentence, we&apos;ll try to predict the third word using a fixed window NLM</span>

<span class="hljs-comment"># before we start any modeling, we have to tokenize our input and convert the words to indices</span>

vocab = {} <span class="hljs-comment"># map from word type to index &#x7EDF;&#x8BA1;&#x51FA;&#x6240;&#x6709;&#x4E0D;&#x540C;&#x8F93;&#x5165;&#x7684;&#x8BCD;</span>
inputs = [] <span class="hljs-comment"># store an indexified version of each sentence &#x5217;&#x51FA;input&#x7684;&#x8BCD;&#x7684;&#x7F16;&#x53F7;</span>

<span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sentences:
  sent_idxes = []

  words = sent.split()
  <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> words:
    <span class="hljs-keyword">if</span> w <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> vocab:
      vocab[w] = len(vocab) <span class="hljs-comment"># add a new word type</span>
    sent_idxes.append(vocab[w])

  inputs.append(sent_idxes)
</code></pre>
<blockquote>
<p>vocab&#xFF1A; {&apos;bob&apos;: 0, &apos;likes&apos;: 1, &apos;sheep&apos;: 2, &apos;alice&apos;: 3, &apos;is&apos;: 4, &apos;fast&apos;: 5, &apos;cs685&apos;: 6, &apos;fun&apos;: 7, &apos;i&apos;: 8, &apos;love&apos;: 9, &apos;lamp&apos;: 10}</p>
<p>inputs&#xFF1A; [[0, 1, 2], [3, 4, 5], [6, 4, 7], [8, 9, 10]]</p>
</blockquote>
<h2 id="&#x5206;&#x51FA;label&#x548C;prefixes&#xFF08;input&#xFF09;">&#x5206;&#x51FA;Label&#x548C;prefixes&#xFF08;input&#xFF09;</h2>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torch

<span class="hljs-comment"># two things:input</span>
<span class="hljs-comment"># 1. convert to LongTensor</span>
<span class="hljs-comment"># 2. define inputs/outputs, the first two words and the third word</span>
prefixes = torch.LongTensor([sent[:<span class="hljs-number">2</span>] <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> inputs])
labels = torch.LongTensor([sent[<span class="hljs-number">2</span>] <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> inputs])
</code></pre>
<h2 id="&#x6784;&#x5EFA;&#x7F51;&#x7EDC;">&#x6784;&#x5EFA;&#x7F51;&#x7EDC;</h2>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">NLM</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-comment"># two things you need to do</span>
    <span class="hljs-comment"># 1. init function (initializes all the **params** of the network)</span>
    <span class="hljs-comment"># 2. forward function (defines the forward computations)</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, d_embedding, d_hidden, window_size, len_vocab)</span>:</span>
        super(NLM, self).__init__() <span class="hljs-comment"># initialize the base Module class</span>
        self.d_embs = d_embedding 
        self.embeds = nn.Embedding(len_vocab, d_embedding)
        <span class="hljs-comment"># concatenate embeddings &gt; hidden</span>
        self.W_hid = nn.Linear(d_embedding * window_size, d_hidden)
        <span class="hljs-comment"># hidden &gt; output probability distribution over vocab</span>
        self.W_out = nn.Linear(d_hidden, len_vocab)



    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, input)</span>:</span> <span class="hljs-comment"># each input will be a batch of prefixes</span>
        batch_size, window_size = input.size()
        embs = self.embeds(input) <span class="hljs-comment"># 4 x 2 x 5</span>

        <span class="hljs-comment"># next, concatenate the prefix embeddings together</span>
        concat_embs = embs.view(batch_size, window_size * self.d_embs) <span class="hljs-comment"># 4 x 10</span>

        <span class="hljs-comment"># we project this to the hidden space</span>
        hiddens = self.W_hid(concat_embs) <span class="hljs-comment"># 4 x d_hidden</span>
        <span class="hljs-comment"># finally, project hiddens to vocabulary space</span>
        outs = self.W_out(hiddens)


        <span class="hljs-comment"># probs = nn.functional.softmax(outs, dim=1)</span>

        <span class="hljs-keyword">return</span> outs <span class="hljs-comment"># return unnormalized probability, alsk known as &quot;logits&quot;</span>

network = NLM(d_embedding=<span class="hljs-number">5</span>, d_hidden=<span class="hljs-number">12</span>, window_size=<span class="hljs-number">2</span>, len_vocab=len(vocab))
network(prefixes)
</code></pre>
<blockquote>
<p>tensor([[-0.0430,  0.3760, -0.0404,  0.3504,  0.4570, -0.2930, -0.4358,  0.1522,          0.0560, -0.0793, -0.3191],        [-0.4502, -0.2621,  0.2912,  0.1220,  0.3365, -0.4783, -0.0571, -0.1595,          0.2648, -0.5140, -0.3851],        [-0.3595, -0.0772,  0.2197,  0.2927,  0.2890, -0.2596, -0.1193, -0.2810,         -0.0440, -0.7078, -0.5025],        [-0.0506, -0.3506,  0.4256,  0.5166,  0.5341, -0.3084,  0.2623, -0.0991,          0.0066,  0.0313, -0.0552]], grad_fn=<addmmbackward>)</addmmbackward></p>
</blockquote>
<h2 id="&#x8BBE;&#x7F6E;&#x8D85;&#x53C2;&#x6570;">&#x8BBE;&#x7F6E;&#x8D85;&#x53C2;&#x6570;</h2>
<pre><code class="lang-python">num_epochs = <span class="hljs-number">30</span>
learning_rate = <span class="hljs-number">0.1</span>

loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(params=network.parameters(), lr=learning_rate)
</code></pre>
<h2 id="&#x8BAD;&#x7EC3;&#x6A21;&#x578B;">&#x8BAD;&#x7EC3;&#x6A21;&#x578B;</h2>
<pre><code class="lang-python"><span class="hljs-comment"># training loop</span>
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(num_epochs):
    logits = network(prefixes)
    loss = loss_fn(logits, labels)
    print(f<span class="hljs-string">&apos;epochs[{i+1}/{num_epochs}]loss: {loss.item():.4f}&apos;</span>)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
</code></pre>
<h2 id="&#x9884;&#x6D4B;&#x7ED3;&#x679C;">&#x9884;&#x6D4B;&#x7ED3;&#x679C;</h2>
<pre><code class="lang-python">rev_vocab = dict((idx, word) <span class="hljs-keyword">for</span> (word, idx) <span class="hljs-keyword">in</span> vocab.items())
boblikes = prefixes[<span class="hljs-number">0</span>].unsqueeze(<span class="hljs-number">0</span>)
logits = network(boblikes)
probs = nn.functional.softmax(logits, dim=<span class="hljs-number">1</span>).squeeze()

argmax_idx = torch.argmax(probs).item()
print(probs)
print(argmax_idx)
print(f<span class="hljs-string">&apos;given &quot;bob likes&quot;, the model prediction as next word  is: [{rev_vocab[argmax_idx]}], probability is {probs[argmax_idx]}&apos;</span>)
</code></pre>
<blockquote>
<p>tensor([1.6756e-03, 2.5217e-03, 9.7605e-01, 1.0478e-03, 2.4076e-03, 6.0406e-03,        2.2282e-03, 4.8825e-04, 2.3747e-03, 3.0853e-03, 2.0833e-03],       grad_fn=<squeezebackward0>) 2 given &quot;bob likes&quot;, the model prediction as next word  is: [sheep], probability is 0.9760469198226929</squeezebackward0></p>
</blockquote>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="../3 Backpropagation/Backpropagation(补充" class="navigation navigation-prev " aria-label="Previous page: Backpropagation(补充)">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="../5 Attention mechanisms/Attention mechanisms.html" class="navigation navigation-next " aria-label="Next page: Attention mechanisms">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"NLM implementation","level":"1.2.5","depth":2,"next":{"title":"Attention mechanisms","level":"1.2.6","depth":2,"path":"5 Attention mechanisms/Attention mechanisms.md","ref":"5 Attention mechanisms/Attention mechanisms.md","articles":[]},"previous":{"title":"Backpropagation(补充)","level":"1.2.4.1","depth":3,"path":"3 Backpropagation/Backpropagation(补充","ref":"3 Backpropagation/Backpropagation(补充","articles":[]},"dir":"ltr"},"config":{"plugins":["back-to-top-button","katex","chapter-fold","code","splitter","-lunr","-search","search-pro","insert-logo"],"styles":{"website":"styles/website.css"},"pluginsConfig":{"chapter-fold":{},"splitter":{},"search-pro":{},"code":{"copyButtons":true},"katex":{},"fontsettings":{"theme":"white","family":"sans","size":2},"highlight":{},"back-to-top-button":{},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false},"insert-logo":{"style":"background: none; max-height: 30px; min-height: 30px","url":"img/logo.png"}},"theme":"default","author":"Ivan Mao","pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"variables":{},"title":"Gitbook使用教程","language":"zh-hans","gitbook":"*","description":"学习Gitbook时总结的笔记"},"file":{"path":"4 NLM implementation/NLM implementation.md","mtime":"2022-12-24T04:54:39.452Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2023-01-31T04:26:06.263Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="../gitbook/gitbook-plugin-back-to-top-button/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-chapter-fold/chapter-fold.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-code/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-splitter/splitter.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search-pro/jquery.mark.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search-pro/search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-insert-logo/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>


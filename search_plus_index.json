{"./":{"url":"./","title":"NLP","keywords":"","body":"这是我通过 UMass马萨诸塞 CS685-自然语言处理进阶课程后整理的笔记 课程主页：Advanced NLP - CS 685, Spring 2022, UMass Amherst 笔记主页：Notes "},"0 Intro/Intro.html":{"url":"0 Intro/Intro.html","title":"Introduction","keywords":"","body":"Course Introduction Natural Language Processing languages that evolved naturally through human use e.g., Spanish, English, Arabic, Hindi, etc. NOT: controlled languages (e.g., Klingon); programming languages Levels of linguistic structure Morphology: 词法 => Sub-word Supervised learning: Given a collection of labeled examples (each example is a document X paired with a label Y), learn a mapping from X to Y Tasks commonly tackled in a supervised setting: Sentiment analysis: map a product review to a sentiment label (positive or negative) Question answering: given a question about a document, provide the location of the answer within the document Textual entailment: given two sentences, identify whether the first sentence entails or contradicts the second one Machine translation: given a sentence in a source language, produce a translation of that sentence in a target language Self-supervised learning: given a collection of just text (no extra labels), create labels out of the text and use them for representation learning Language modeling: given the beginning of a sentence or document, predict the next word Masked language modeling: given an entire document with some words or spans masked out, predict the missing words Representation learning: given some text, create a representation of that text (e.g., real-valued, low-dimensional vectors) that capture its linguistic properties (syntax, semantics) Transfer learning: pretrain a large selfsupervised model, and then fine-tune it on a small downstream supervised dataset Transfer learning has recently (last ~2 years) become the method of choice for most downstream NLP tasks. Consequently, most of this class will focus on new research in transfer learning for NLP! "},"1 Language Modeling/1 Language Modeling.html":{"url":"1 Language Modeling/1 Language Modeling.html","title":"Language Modeling","keywords":"","body":"Language Modeling Let’s say I want to train a model for sentiment analysis. Nowadays, we use transfer learning: This lecture: language modeling, which forms the core of most self-supervised NLP approaches Probabilistic Language Modeling Goal: compute the probability of a sentence or sequence of words: P(W) = P(w1,w2,w3,w4,w5…wn) Related task: probability of an upcoming word: P(w5|w1,w2,w3,w4) A model that computes either of these: P(W) or P(wn|w1,w2…wn-1) is called a language model or LM How to compute P(W) P(A,B)=P(A)P(B∣A)\r P(A,B) = P(A)P(B|A)\r P(A,B)=P(A)P(B∣A) P(x1,x2,x3,xn)=P(x1)P(x2∣x1)P(x3∣x1,x2),P(xn∣x1,xn−1)P(x_{1},x_{2},x_{3}, x_{n}) = P(x_{1})P(x_{2}|x_{1})P(x_{3}|x_{1},x_{2}), P(x_{n}|x_{1}, x_{n-1})P(x​1​​,x​2​​,x​3​​,x​n​​)=P(x​1​​)P(x​2​​∣x​1​​)P(x​3​​∣x​1​​,x​2​​),P(x​n​​∣x​1​​,x​n−1​​) P(“its water is so transparent”) = P(its) × P(water|its) × P(is|its water) × P(so|its water is) × P(transparent|its water is so) How to estimate these probabilities P(the | its water is so transparent that) = Count(its water is so transparent that the) / Count(its water is so transparent that) Too many possible sentences! We’ll never see enough data for estimating these Markov Assumption Simplifying assumption: P(the | its water is so transparent that) ≈ P(the | that) P(the | its water is so transparent that) ≈ P(the | transparent that) Simplest case: Unigram model N-gram models We can extend to trigrams, 4-grams, 5-grams In general this is an insufficient model of language, because language has long-distance dependencies: “The computer which I had just put into the machine room on the fifth floor crashed.” But we can often get away with N-gram models Estimating bigram probabilities The Maximum Likelihood Estimate (MLE): relative frequency based on the empirical counts on a training set these probabilities get super tiny when we have longer inputs w/ more infrequent words… how can we get around this? logs to avoid underflow We can’t allow test sentences into the training set. We will assign it an artificially high probability when we set it in the test* set Perplexity "},"2 Neural Language Models/Neural Language Models.html":{"url":"2 Neural Language Models/Neural Language Models.html","title":"Neural Language Models","keywords":"","body":"Neural Language Models one-hot vectors Shortcoming: all words are equally (dis)similar: dot product is zero! these vectors are orthogonal Neural networks embeddings: represent words with low-dimensional vectors neural networks compose word embeddings into vectors for phrases, sentences, and documents Softmax layer: convert a vector representation into a probability distribution over the entire vocabulary Each row of W contains feature weights for a corresponding word in the vocabulary. Each dimension of x corresponds to a feature of the prefix Composition functions A fixed-window neural Language Model how does this compare to a normal n-gram model? Improvements over n-gram LM: No sparsity problem Model size is O(n) not O(exp(n)) Remaining problems: Fixed window is too small Enlarging window enlarges Window can never be large enough! Each ci uses different rows of W. We don’t share weights across the window. Recurrent Neural Networks RNN Advantages: Can process any length input Model size doesn’t increase for longer input Computation for step t can (in theory) use information from many steps back Weights are shared across timestep -> representations are shared RNN Disadvantages: Recurrent computation is slow In practice, difficult to access information from many steps back "},"3 Backpropagation/Backpropagation.html":{"url":"3 Backpropagation/Backpropagation.html","title":"Backpropagation","keywords":"","body":"Backpropagation How do we train to yield great prediction? Define a loss function L(θ)L(\\theta)L(θ) Calculate the gradient of L(θ)L(\\theta)L(θ) Take a step in the direction of the negative gradient: Minimizing the loss: θnew=θold−ndLdθ\\theta_{new} = \\theta_{old} - n\\frac{dL}{d\\theta}θ​new​​=θ​old​​−n​dθ​​dL​​ Example: Single Neuron STEP1: Forward propagation compute the value of each neuron h, o using the model equations predict the value of y: o Goal: make o as close to y as possible STEP2: Compute loss function Using square loss in this example for simplicity L=12(y−o)2L = \\frac{1}{2}(y -o )^{2}L=​2​​1​​(y−o)​2​​ STEP3: Compute gradient weight θ\\thetaθ Start with dLdwL\\frac{dL}{dw_{L}}​dw​L​​​​dL​​ "},"4 NLM implementation/NLM implementation.html":{"url":"4 NLM implementation/NLM implementation.html","title":"NLM implementation","keywords":"","body":"NLM implementation 输入文本 sentences = [ 'bob likes sheep', 'alice is fast', 'cs685 is fun', 'i love lamp' ] 列举对应的编号 # given the first two words of each sentence, we'll try to predict the third word using a fixed window NLM # before we start any modeling, we have to tokenize our input and convert the words to indices vocab = {} # map from word type to index 统计出所有不同输入的词 inputs = [] # store an indexified version of each sentence 列出input的词的编号 for sent in sentences: sent_idxes = [] words = sent.split() for w in words: if w not in vocab: vocab[w] = len(vocab) # add a new word type sent_idxes.append(vocab[w]) inputs.append(sent_idxes) vocab： {'bob': 0, 'likes': 1, 'sheep': 2, 'alice': 3, 'is': 4, 'fast': 5, 'cs685': 6, 'fun': 7, 'i': 8, 'love': 9, 'lamp': 10} inputs： [[0, 1, 2], [3, 4, 5], [6, 4, 7], [8, 9, 10]] 分出Label和prefixes（input） import torch # two things:input # 1. convert to LongTensor # 2. define inputs/outputs, the first two words and the third word prefixes = torch.LongTensor([sent[:2] for sent in inputs]) labels = torch.LongTensor([sent[2] for sent in inputs]) 构建网络 import torch.nn as nn class NLM(nn.Module): # two things you need to do # 1. init function (initializes all the **params** of the network) # 2. forward function (defines the forward computations) def __init__(self, d_embedding, d_hidden, window_size, len_vocab): super(NLM, self).__init__() # initialize the base Module class self.d_embs = d_embedding self.embeds = nn.Embedding(len_vocab, d_embedding) # concatenate embeddings > hidden self.W_hid = nn.Linear(d_embedding * window_size, d_hidden) # hidden > output probability distribution over vocab self.W_out = nn.Linear(d_hidden, len_vocab) def forward(self, input): # each input will be a batch of prefixes batch_size, window_size = input.size() embs = self.embeds(input) # 4 x 2 x 5 # next, concatenate the prefix embeddings together concat_embs = embs.view(batch_size, window_size * self.d_embs) # 4 x 10 # we project this to the hidden space hiddens = self.W_hid(concat_embs) # 4 x d_hidden # finally, project hiddens to vocabulary space outs = self.W_out(hiddens) # probs = nn.functional.softmax(outs, dim=1) return outs # return unnormalized probability, alsk known as \"logits\" network = NLM(d_embedding=5, d_hidden=12, window_size=2, len_vocab=len(vocab)) network(prefixes) tensor([[-0.0430, 0.3760, -0.0404, 0.3504, 0.4570, -0.2930, -0.4358, 0.1522, 0.0560, -0.0793, -0.3191], [-0.4502, -0.2621, 0.2912, 0.1220, 0.3365, -0.4783, -0.0571, -0.1595, 0.2648, -0.5140, -0.3851], [-0.3595, -0.0772, 0.2197, 0.2927, 0.2890, -0.2596, -0.1193, -0.2810, -0.0440, -0.7078, -0.5025], [-0.0506, -0.3506, 0.4256, 0.5166, 0.5341, -0.3084, 0.2623, -0.0991, 0.0066, 0.0313, -0.0552]], grad_fn=) 设置超参数 num_epochs = 30 learning_rate = 0.1 loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(params=network.parameters(), lr=learning_rate) 训练模型 # training loop for i in range(num_epochs): logits = network(prefixes) loss = loss_fn(logits, labels) print(f'epochs[{i+1}/{num_epochs}]loss: {loss.item():.4f}') optimizer.zero_grad() loss.backward() optimizer.step() 预测结果 rev_vocab = dict((idx, word) for (word, idx) in vocab.items()) boblikes = prefixes[0].unsqueeze(0) logits = network(boblikes) probs = nn.functional.softmax(logits, dim=1).squeeze() argmax_idx = torch.argmax(probs).item() print(probs) print(argmax_idx) print(f'given \"bob likes\", the model prediction as next word is: [{rev_vocab[argmax_idx]}], probability is {probs[argmax_idx]}') tensor([1.6756e-03, 2.5217e-03, 9.7605e-01, 1.0478e-03, 2.4076e-03, 6.0406e-03, 2.2282e-03, 4.8825e-04, 2.3747e-03, 3.0853e-03, 2.0833e-03], grad_fn=) 2 given \"bob likes\", the model prediction as next word is: [sheep], probability is 0.9760469198226929 "},"5 Attention mechanisms/Attention mechanisms.html":{"url":"5 Attention mechanisms/Attention mechanisms.html","title":"Attention mechanisms","keywords":"","body":"Attention mechanisms Cons of RNN RNNs suffer from a bottleneck problem: The current hidden representation must encode all of the information about the text observed so far This becomes difficult especially with longer sequences (a vector to present whole sentence) “you can’t cram the meaning of a whole %&@#&ing sentence into a single $*(&@ing vector!” ​ — Ray Mooney (NLP professor at UT Austin) what if we use multiple vectors? Attention Attention mechanisms (Bahdanau et al.,2015）allow language models to focus on a particular part of the observed context at each time step Originally developed for machine translation, and intuitively similar to word alignments between different languages In general, we have a single query vector and multiple key vectors. We want to score each query-key pair Attention solves the bottleneck problem Attention allows decoder to look directly at source; bypass bottleneck Attention helps with vanishing gradient problem Provides shortcut to faraway states Attention provides some interpretability By inspecting attention distribution, we can see what the decoder was focusing on We get alignment for free! This is cool because we never explicitly trained an alignment system The network just learned alignment by itself Many variants of attention Self-attention can completely replace recurrence! Each element in the sentence attends to the other elements Q=Wq⋅PQ = W^{q}\\cdot P Q=W​q​​⋅P K=Wk⋅PK = W^{k}\\cdot P K=W​k​​⋅P A=Q⋅KA = Q \\cdot KA=Q⋅K 计算出相关性 M=A(P)VM = A(P)VM=A(P)V 计算出最后的embedding 如果有位置掩码P，则乘其值 Multi-head self-attention 寻求不同的相关性 "},"6 Transformer and sequence-to sequence learning/Transformer and sequence-to sequence learning.html":{"url":"6 Transformer and sequence-to sequence learning/Transformer and sequence-to sequence learning.html","title":"Transformer and sequence-to sequence learning","keywords":"","body":"Transformer and sequence-to sequence learning sequence-to-sequence learning Used when inputs and outputs are both sequences of words (e.g., machine translation, summarization) we’ll use French (f) to English (e) as a running example goal: given French sentence f with tokens f1, f2, … fn produce English translation e with tokens e1, e2, … em real goal: compute argmaxep(e∣f)\\arg\\max_{e} p(e|f)argmax​e​​p(e∣f) seq2seq models use two different neural networks to model first we have the encoder, which encodes the French sentence f then, we have the decoder, which produces the English sentence e4 Neural Machine Translation (NMT) Training a Neural Machine Translation system Transformer Position embeddings are added to each word embedding. Otherwise, since we have no recurrence, our model is unaware of the position of a word in the sequence! Residual connections, which mean that we add the input to a particular block to its output, help improve gradient flow A feed-forward layer on top of the attention weighted averaged value vectors allows us to add more parameters / nonlinearity Moving onto the decoder, which takes in English sequences that have been shifted to the right (e.g., schools opened their) the decoder is responsible for predicting the English words, we need to apply masking as we saw before. we have cross attention, which connects the decoder to the encoder by enabling it to attend over the encoder’s final hidden states. output embeding是ground truth的embedding结果，（teacher forcing：use ground truth as input）也就是正确答案，那存在一个问题：在testing是output embedding怎么输入，不知道正确答案？解决办法：training时，把正确结果当做decoder的输入，在testing时，把上一个时刻decoder的结果当做当前时刻的输入，但是这样在decoder的输出结果中，肯定存在错误，所以不能让decoder在训练时只看到正确结果，就需要在training时人为加入一些错误的结果。 Positional encoding "},"7 Transfer learning with neural language models/Transfer learning with neural language models.html":{"url":"7 Transfer learning with neural language models/Transfer learning with neural language models.html","title":"Transfer learning with neural language models","keywords":"","body":"Transfer learning with neural language models What is transfer learning In our context: take a network trained on a task for which it is easy to generate labels, and adapt it to a different task for which it is harder. In computer vision: train a CNN on ImageNet, transfer its representations to every other CV task In NLP: train a really big language model on billions of words, transfer to every NLP task! Contextual Representations History of Contextual Representations ELMo representations are contextual: they depend on the entire sentence in which a word is used. BERT Problem with Previous Methods Language models only use left context or right context, but language understanding is bidirectional Masked LM Model Architecture Fine-Tuning Procedure Develop History "},"8 HuggingFace/HuggingFace.html":{"url":"8 HuggingFace/HuggingFace.html","title":"HuggingFace","keywords":"","body":"HuggingFace 安装 环境 python=3.6 pytorch=1.10 安装transformers #pip安装 #pip install transformers #conda安装 #conda install -c huggingface transformers 安装datasets #pip安装 #pip install datasets #conda安装 #conda install -c huggingface -c conda-forge datasets tokenizer 加载预训练字典和分词方法 from transformers import BertTokenizer tokenizer = BertTokenizer.from_pretrained( pretrained_model_name_or_path='bert-base-chinese', cache_dir=None, force_download=False, ) sents = [ '选择珠江花园的原因就是方便。', '笔记本的键盘确实爽。', '房间太小。其他的都一般。', '今天才知道这书还有第6卷,真有点郁闷.', '机器背面似乎被撕了张什么标签，残胶还在。', ] 编码 out = tokenizer.encode( text=sents[0], text_pair=sents[1], #当句子长度大于max_length时,截断 truncation=True, #一律补pad到max_length长度 padding='max_length', add_special_tokens=True, max_length=30, return_tensors=None, ) print(out) tokenizer.decode(out) 执行后结果 [101, 6848, 2885, 4403, 3736, 5709, 1736, 4638, 1333, 1728, 2218, 3221, 3175, 912, 511, 102, 5011, 6381, 3315, 4638, 7241, 4669, 4802, 2141, 4272, 511, 102, 0, 0, 0] '[CLS] 选 择 珠 江 花 园 的 原 因 就 是 方 便 。 [SEP] 笔 记 本 的 键 盘 确 实 爽 。 [SEP] [PAD] [PAD] [PAD]' 增强的编码函数 out = tokenizer.encode_plus( text=sents[0], text_pair=sents[1], #当句子长度大于max_length时,截断 truncation=True, #一律补零到max_length长度 padding='max_length', max_length=30, add_special_tokens=True, #可取值tf,pt,np,默认为返回list return_tensors=None, #返回token_type_ids return_token_type_ids=True, #返回attention_mask return_attention_mask=True, #返回special_tokens_mask 特殊符号标识 return_special_tokens_mask=True, #返回offset_mapping 标识每个词的起止位置,这个参数只能BertTokenizerFast使用 #return_offsets_mapping=True, #返回length 标识长度 return_length=True, ) #input_ids 就是编码后的词 #token_type_ids 第一个句子和特殊符号的位置是0,第二个句子的位置是1 #special_tokens_mask 特殊符号的位置是1,其他位置是0 #attention_mask pad的位置是0,其他位置是1 #length 返回句子长度 for k, v in out.items(): print(k, ':', v) tokenizer.decode(out['input_ids']) 执行后结果 input_ids : [101, 6848, 2885, 4403, 3736, 5709, 1736, 4638, 1333, 1728, 2218, 3221, 3175, 912, 511, 102, 5011, 6381, 3315, 4638, 7241, 4669, 4802, 2141, 4272, 511, 102, 0, 0, 0] token_type_ids : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0] special_tokens_mask : [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1] attention_mask : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0] length : 30 '[CLS] 选 择 珠 江 花 园 的 原 因 就 是 方 便 。 [SEP] 笔 记 本 的 键 盘 确 实 爽 。 [SEP] [PAD] [PAD] [PAD]' 批量编码句子 out = tokenizer.batch_encode_plus( batch_text_or_text_pairs=[sents[0], sents[1]], add_special_tokens=True, #当句子长度大于max_length时,截断 truncation=True, #一律补零到max_length长度 padding='max_length', max_length=15, #可取值tf,pt,np,默认为返回list return_tensors=None, #返回token_type_ids return_token_type_ids=True, #返回attention_mask return_attention_mask=True, #返回special_tokens_mask 特殊符号标识 return_special_tokens_mask=True, #返回offset_mapping 标识每个词的起止位置,这个参数只能BertTokenizerFast使用 #return_offsets_mapping=True, #返回length 标识长度 return_length=True, ) #input_ids 就是编码后的词 #token_type_ids 第一个句子和特殊符号的位置是0,第二个句子的位置是1 #special_tokens_mask 特殊符号的位置是1,其他位置是0 #attention_mask pad的位置是0,其他位置是1 #length 返回句子长度 for k, v in out.items(): print(k, ':', v) tokenizer.decode(out['input_ids'][0]), tokenizer.decode(out['input_ids'][1]) 执行后结果 input_ids : [[101, 6848, 2885, 4403, 3736, 5709, 1736, 4638, 1333, 1728, 2218, 3221, 3175, 912, 102], [101, 5011, 6381, 3315, 4638, 7241, 4669, 4802, 2141, 4272, 511, 102, 0, 0, 0]] token_type_ids : [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]] special_tokens_mask : [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]] length : [15, 12] attention_mask : [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]] ('[CLS] 选 择 珠 江 花 园 的 原 因 就 是 方 便 [SEP]', '[CLS] 笔 记 本 的 键 盘 确 实 爽 。 [SEP] [PAD] [PAD] [PAD]') 批量编码成对的句子 out = tokenizer.batch_encode_plus( batch_text_or_text_pairs=[(sents[0], sents[1]), (sents[2], sents[3])], add_special_tokens=True, #当句子长度大于max_length时,截断 truncation=True, #一律补零到max_length长度 padding='max_length', max_length=30, #可取值tf,pt,np,默认为返回list return_tensors=None, #返回token_type_ids return_token_type_ids=True, #返回attention_mask return_attention_mask=True, #返回special_tokens_mask 特殊符号标识 return_special_tokens_mask=True, #返回offset_mapping 标识每个词的起止位置,这个参数只能BertTokenizerFast使用 #return_offsets_mapping=True, #返回length 标识长度 return_length=True, ) #input_ids 就是编码后的词 #token_type_ids 第一个句子和特殊符号的位置是0,第二个句子的位置是1 #special_tokens_mask 特殊符号的位置是1,其他位置是0 #attention_mask pad的位置是0,其他位置是1 #length 返回句子长度 for k, v in out.items(): print(k, ':', v) tokenizer.decode(out['input_ids'][0]) 执行后结果 input_ids : [[101, 6848, 2885, 4403, 3736, 5709, 1736, 4638, 1333, 1728, 2218, 3221, 3175, 912, 511, 102, 5011, 6381, 3315, 4638, 7241, 4669, 4802, 2141, 4272, 511, 102, 0, 0, 0], [101, 2791, 7313, 1922, 2207, 511, 1071, 800, 4638, 6963, 671, 5663, 511, 102, 791, 1921, 2798, 4761, 6887, 6821, 741, 6820, 3300, 5018, 127, 1318, 117, 4696, 3300, 102]] token_type_ids : [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]] special_tokens_mask : [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]] length : [27, 30] attention_mask : [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]] '[CLS] 选 择 珠 江 花 园 的 原 因 就 是 方 便 。 [SEP] 笔 记 本 的 键 盘 确 实 爽 。 [SEP] [PAD] [PAD] [PAD]' 获取字典 zidian = tokenizer.get_vocab() type(zidian), len(zidian), '月光' in zidian, (dict, 21128, False) #添加新词 tokenizer.add_tokens(new_tokens=['月光', '希望']) #添加新符号 tokenizer.add_special_tokens({'eos_token': '[EOS]'}) zidian = tokenizer.get_vocab() type(zidian), len(zidian), zidian['月光'], zidian['[EOS]'] 执行后 (dict, 21131, 21128, 21130) Datasets 加载数据 from datasets import load_dataset #注意：如果你的网络不允许你执行这段的代码，则直接运行【从磁盘加载数据】即可，我已经给你准备了本地化的数据文件 dataset = load_dataset(path='seamew/ChnSentiCorp') #保存数据集到磁盘 #注意：运行这段代码要确保【加载数据】运行是正常的，否则直接运行【从磁盘加载数据】即可 dataset.save_to_disk(dataset_dict_path='./data/ChnSentiCorp') #从磁盘加载数据 from datasets import load_from_disk dataset = load_from_disk('./data/ChnSentiCorp') dataset 数据集函数 sort #未排序的label是乱序的 print(dataset['label'][:10]) #排序之后label有序了 sorted_dataset = dataset.sort('label') print(sorted_dataset['label'][:10]) print(sorted_dataset['label'][-10:]) shuffle #打乱顺序 shuffled_dataset = sorted_dataset.shuffle(seed=42) shuffled_dataset['label'][:10] select dataset.select([0, 10, 20, 30, 40, 50]) filter def f(data): return data['text'].startswith('选择') start_with_ar = dataset.filter(f) len(start_with_ar), start_with_ar['text'] train_test_split #train_test_split, 切分训练集和测试集 dataset.train_test_split(test_size=0.1) shard #把数据切分到4个桶中,均匀分配 dataset.shard(num_shards=4, index=0) rename_column #rename_column dataset.rename_column('text', 'textA') remove_columns #remove_columns dataset.remove_columns(['text']) map def f(data): data['text'] = 'My sentence: ' + data['text'] return data datatset_map = dataset.map(f) datatset_map['text'][:5] set_format dataset.set_format(type='torch', columns=['label']) dataset[0] 导出/加载 #导出为csv格式 dataset = load_dataset(path='seamew/ChnSentiCorp', split='train') dataset.to_csv(path_or_buf='./data/ChnSentiCorp.csv') #加载csv格式数据 csv_dataset = load_dataset(path='csv', data_files='./data/ChnSentiCorp.csv', split='train') csv_dataset[20] #导出为json格式 dataset = load_dataset(path='seamew/ChnSentiCorp', split='train') dataset.to_json(path_or_buf='./data/ChnSentiCorp.json') #加载json格式数据 json_dataset = load_dataset(path='json', data_files='./data/ChnSentiCorp.json', split='train') json_dataset[20] Metric 列出评价指标 from datasets import list_metrics #列出评价指标 metrics_list = list_metrics() len(metrics_list), metrics_list 执行后结果 (34, ['accuracy', 'bertscore', 'bleu', 'bleurt', 'cer', 'chrf', 'code_eval', 'comet', 'competition_math', 'coval', 'cuad', 'f1', 'gleu', 'glue', 'google_bleu', 'indic_glue', 'matthews_correlation', 'mauve', 'meteor', 'pearsonr', 'precision', 'recall', 'rouge', 'sacrebleu', 'sari', 'seqeval', 'spearmanr', 'squad', 'squad_v2', 'super_glue', 'ter', 'wer', 'wiki_split', 'xnli']) from datasets import load_metric #加载一个评价指标 metric = load_metric('glue', 'mrpc') print(metric.inputs_description) #计算一个评价指标 predictions = [0, 1, 0] references = [0, 1, 1] final_score = metric.compute(predictions=predictions, references=references) final_score "}}